{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DP Primer",
      "provenance": [],
      "authorship_tag": "ABX9TyM21Mc33+hMB87BHWStWOgT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kritikalcoder/DP-primer/blob/master/DP_Primer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkPm9nAgmza5",
        "colab_type": "text"
      },
      "source": [
        "# **Differential Privacy Primer**\n",
        "\n",
        "## **Overview**\n",
        "\n",
        "### Need for Privacy \n",
        "\n",
        "### Earlier Approaches to Privacy\n",
        "\n",
        "### Motivating Use Case \n",
        "\n",
        "### Differential Privacy\n",
        "- Randomized Algorithm\n",
        "- Informal Explanation\n",
        "- Computation - Privacy - Accuracy Trade-off\n",
        "- Formal Definitions\n",
        "- Privacy Budget\n",
        "- Neighbouring Databases\n",
        "- Function/Query Sensitivity\n",
        "- Adversary\n",
        "- Scaling with Query Composition\n",
        "- Noise Mechanisms\n",
        "- Local DP\n",
        "- Global DP\n",
        "- Query Composition  \n",
        "- Moments Accountant\n",
        "- Simple Use Case\n",
        "\n",
        "### Applications\n",
        "\n",
        "### DP in Machine Learning\n",
        "- DP Classification  \n",
        "- DP Regression  \n",
        "- DP Clustering  \n",
        "\n",
        "### DP at OpenMined\n",
        "- Wrappers for existing libraries  \n",
        "- DifferentialPrivacy.ts\n",
        "\n",
        "### DP in SQL queries\n",
        "\n",
        "### Practical DP\n",
        "- Side-channel attacks  \n",
        "\n",
        "### Things no one tells you about DP\n",
        "- Bounds on data\n",
        "- Bounds on noise distribution\n",
        "\n",
        "### DP Blog Series\n",
        "List of blog topics here\n",
        "\n",
        "### Glossary\n",
        "\n",
        "### Resources"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOOxG8WgoKna",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3FQLEg0oLF2",
        "colab_type": "text"
      },
      "source": [
        "## **Need for Privacy**\n",
        "\n",
        "blah blah, fill in later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VZUgPq4pCnt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-R-CiaLZeE6",
        "colab_type": "text"
      },
      "source": [
        "## **Earlier Approaches to Privacy**\n",
        "\n",
        "### **Data Perturbation**\n",
        "Data perturbation approaches can be grouped into two main categories: the probability distribution approach and the value distortion approach. The probability distribution approach replaces the data with another sample from the same (or estimated) distribution or by the distribution itself, and the value distortion approach perturbs data elements or attributes directly by either additive noise, multiplicative noise, or some other randomization procedures.   \n",
        "\n",
        "### **Distributed ML**\n",
        "Extraction of patterns at a given node in a distributed network by exchanging only the minimal necessary information among the participating nodes.  \n",
        "\n",
        "### **Secure Multi-Party Computation**\n",
        "Evaluate a function of the secret inputs coming from two or more input sources, such that no party learns anything more than the expected output of the function.  \n",
        "\n",
        "### **Homomorphic Encryption**\n",
        "Homomorphic Encryption is a special case of a Secure Multi-Party Computation, where an encryption transformation is applied to the data, such that certain distances and operations on the data are preserved in the encrypted space.   \n",
        "\n",
        "### **Data Swapping**\n",
        "Transform the database by switching a subset of attributes between selected pairs of records such that lower order frequency counts are preserved and data confidentiality is not compromised.  \n",
        "\n",
        "### **$k$-Anonymity**\n",
        "The information of each person contained in the publicly released dataset cannot be distinguished from at least $k - 1$ other distinct people.  \n",
        "\n",
        "### **Rule Hiding**\n",
        "Transform the database such that sensitive rules are masked, and at the same time, underlying patterns can still be discovered. Decrease the support of sensitive rules using a user-specified threshold of hiding rules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5CEjnbyZj_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Nr9vARLovW8",
        "colab_type": "text"
      },
      "source": [
        "## **Motivating Use Case**\n",
        "\n",
        "**Netflix challenge shows anonymization is not enough!**\n",
        "\n",
        "Anonymity is not enough. In 2006, Netflix announced a \\$1 million prize challenge to improve their movie recommendation service. They publicly released a large dataset containing approximately 100 million movie ratings created by approximately 480 thousand Netflix subscribers over six years. Netflix claimed to have removed all customer identifying information. But this definition of anonymity is not absolute - they didn't take into account how much a potential adversary (trying to breach the privacy of a user) needs to know about a Netflix subscriber in order to identify his/her record in the dataset. Thus, the question of compromising privacy becomes existential in order to prove that a public release of a dataset does not breach it's participants' privacy and maintains their anonymity.   \n",
        "\n",
        "Shortly after Netflix released it's prize dataset, a linkage attack was created and published which linked the Netflix database with the publicly available view of the IMDB database, resulting in a privacy breach of many of the Netflix subscribers.   \n",
        "\n",
        "Some conventional approaches to privacy are anonymization, sanitization (sampled subset) and controlling access and flow of information. But these do not provide the privacy guarantee.  \n",
        "\n",
        "Privacy of individuals has become a need with rising popularity in today's age of information. This survey of research work over the past few years on differential privacy and privacy preserving machine learning aims to understand the current state of theoretically guaranteed privacy bounds, privacy mechanisms in various scenarios, their implications on the effectiveness of machine learning algorithms and adversarial attacks trying to breach the privacy of participants in a survey or a dataset.   \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ayhqwr8pDSX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NNVcbu2x0lL",
        "colab_type": "text"
      },
      "source": [
        "## **Differential Privacy**\n",
        "\n",
        "\"Differential Privacy (DP) is a definition of privacy that involves making sure a data subject is not affected (e.g. not harmed) by their entry or participation in a database, while maximizing utility/data accuracy (as opposed to random/empty outputs) for queries to a database.\"\n",
        "\n",
        "Differentially private database mechanisms can make confidential data widely available for accurate data analysis, without resorting to data clean rooms, data usage agreements, data protection plans, or restricted views. Differential privacy addresses the paradox of learning nothing about an individual while learning useful information about a population.  \n",
        "\n",
        "Differential privacy guarantees that the impact on the participant of a study is the same independent of whether or not he was in the study. It is the conclusions reached in the study that affect the participant, not his presence or absence in the data set.  \n",
        "\n",
        "Differential privacy is a definition, not an algorithm. For a given computational task T and a given value of $\\epsilon$ there will be many differentially private algorithms for achieving T in an $\\epsilon$-differentially private manner. Some will have better accuracy than others. When $\\epsilon$ is small, finding a highly accurate $\\epsilon$-differentially private algorithm for T can be difficult. Therefore, differential privacy makes it impossible to guess whether one participated in a database with large probability.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mfdsm01Tb0B_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWx3GQyG42NS",
        "colab_type": "text"
      },
      "source": [
        "### **Randomized Algorithm**\n",
        "\n",
        "A randomized algorithm $M$ with domain $A$ and discrete range $B$ is associated with a mapping $M : A \\xrightarrow{} \\Delta(B)$. On input $a \\in A$, the algorithm $M$ outputs $M(a) = b$ with probability $(M(a))_b$ for each $b \\in B$. The probability space is over the coin flips of the algorithm $M$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfZQ2Yh3b0tw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14uhkt3Z42Sq",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "### **Informal Explanation**\n",
        "\n",
        "Modifying the data in a manner such that upon querying the database, no more information about a participant in revealed than the amount revealed if he/she didn't participate in the data survey.\n",
        "\n",
        "\"Adversaries can pinpoint sensitive data of a particular individual using data from multiple databases. For example, researchers were able to find the Governor of Massachusetts's patient profile using de-identified hospital records and a voter registration database. This type of attack is called a linkage attack. Differential privacy helps us avoid linkage attacks by adding noise to true answer of queries to a database. \n",
        "\n",
        "Here's a short, simplified example:\n",
        "\n",
        "1. A data scientist queries a database to find out the number of rows of the database\n",
        "2. The result of the query (i.e. number of rows) is 5\n",
        "3. A differential privacy mechanism adds the number 2 (noise) to the result. \n",
        "4. The final result of the query displayed to the data scientist is the noisy answer 7.\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFWFj_QJb1PD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0mC41aJYMoj",
        "colab_type": "text"
      },
      "source": [
        "### **Computation - Privacy - Accuracy Trade-off**\n",
        "\n",
        "- computational feasibility + speed\n",
        "- privacy guarantee\n",
        "- accuracy of result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LovUU_FHb12V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2wB8n-m42WE",
        "colab_type": "text"
      },
      "source": [
        "### **Formal Definitions**\n",
        "\n",
        "#### **$(\\epsilon, \\delta)$ - DP**\n",
        "A randomized algorithm $M$ with domain $\\mathbb{N}^{|\\chi|}$ is $(\\epsilon, \\delta)-$ differentially private if for all $S \\subseteq$ Range $(M)$ and for all $ x,y \\in N^{|\\chi|}$ such that $||x-y||_1 \\leq 1$:  \n",
        "\\begin{equation}\n",
        "Pr[M(x) \\in S] \\leq exp(\\epsilon) Pr[M(y) \\in S] + \\delta\n",
        "\\end{equation}\n",
        "\n",
        "We are interested in values of $\\delta$ in the order of $1/||x||_1$.  \n",
        " \n",
        "$(\\epsilon, \\delta)$ - differential privacy ensures that for all adjacent $x, y,$ the absolute value of the privacy loss will be bounded by $\\epsilon$ with probability at least $1 - \\delta$. Differential Privacy is immune to post processing.  \n",
        "\n",
        "#### **$(\\epsilon)$ - DP**\n",
        "\n",
        "#### **Renyi - DP**\n",
        "\n",
        "lorem ipsum\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjTGCRcAb2s7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "591f0t9M42eN",
        "colab_type": "text"
      },
      "source": [
        "### **Privacy Budget - $\\epsilon$**\n",
        "\n",
        "lorem  ipsum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGo7G-wnb3XW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezEN8gSeqA-T",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### **Neighbouring Databases**\n",
        "\n",
        "The $l_1$ norm of a database $x$ is denoted $||x||_1$ (number of records in the database) and is defined to be:\n",
        "\\begin{equation}\n",
        "||x||_1 = \\sum_{i=1}^{|\\chi|} |x_i|\n",
        "\\end{equation}\n",
        "The $l_1$ distance between two databases $x$ and $y$ is $||x-y||_1$.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80dftEmTb36Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIdwwdIJrFZW",
        "colab_type": "text"
      },
      "source": [
        "### **Function/Query Sensitivity**\n",
        "\n",
        "The $l_1$-sensitivity of a function $f : \\mathbb{N}^{|\\chi|} \\xrightarrow{} R^k$ is:\n",
        " \\begin{equation}\n",
        " \\Delta f = \\max ||f(x) - f(y)||_1\n",
        " \\end{equation}\n",
        " where $||x-y||_1 = 1$ and $x, y \\in \\mathbb{N}^{|\\chi|}$\n",
        " \n",
        "The $l_1$-sensitivity of a function $f$ captures the magnitude by which a single individual's data can change the function $f$ in the worst case. This gives an upper bound on how much we need to perturb the data to preserve privacy. The $l_1$ sensitivity of counting queries is $1$.   \n",
        "\n",
        "Some of the different types of queries are:  \n",
        "- Small Sensitivity Queries\n",
        "- Large Sensitivity Queries\n",
        "- Numerical Queries (Counting)\n",
        "- Set-Based Queries\n",
        "- Graph-Based Queries\n",
        "- Histograms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA2Mm_SAb4qK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbGS-5X6-zAY",
        "colab_type": "text"
      },
      "source": [
        "### **Noise Mechanisms**\n",
        "\n",
        "#### **Laplacian Mechanism**\n",
        "\n",
        "Given any function $f : \\mathbb{N}^{|\\chi|} \\xrightarrow{} R^k$, the Laplace Mechanism is defined as:\n",
        "\\begin{equation}\n",
        "    M_L(x,f(.),\\epsilon) = f(x) + (Y_1, Y_2, ... , Y_k)\n",
        "\\end{equation}\n",
        "where $Y_i$'s are independently identically distributed (i.i.d) random variables drawn from the Laplacian distribution $Lap(\\Delta f / \\epsilon)$. The Laplace Mechanism preserves $(\\epsilon, 0)$-differential privacy.  \n",
        "\n",
        "- \"Works best for low sensitivity queries \n",
        "- Large epsilon values (aka your privacy budget) are needed to support multiple queries, which can lead to less accurate results.\n",
        "- Negative outputs from the pure Laplacian mechanism are possible, and can be illogical and inconsistent (source: <https://arxiv.org/pdf/1808.10410.pdf>)\"\n",
        "\n",
        "#### **Gaussian Mechanism**\n",
        "\n",
        "- \"The standard deviation is suboptimal in high-privacy regimes, since it grows to infinity.\n",
        "- While the Laplace mechanism can support ε-DP with the relevant standard deviation, the Gaussian mechanism can’t be extended beyond a particular bound. Finding the right noise variance is one solution to this issue.\n",
        "- Source: *Improving the Gaussian Mechanism for Differential Privacy: Analytical Calibration and Optimal Denoising:*[ *http://proceedings.mlr.press/v80/balle18a/balle18a.pdf*](https://slack-redir.net/link?url=http%3A%2F%2Fproceedings.mlr.press%2Fv80%2Fballe18a%2Fballe18a.pdf)\" \n",
        "\n",
        "#### **Exponential Mechanism**\n",
        "\n",
        "The exponential mechanism was designed for situations in which we wish to choose the best response but adding noise directly to the computed quantity can completely destroy its value. For example, setting a price of an auction, where the goal is to maximize the revenue is a highly sensitive value.\n",
        "\n",
        "The exponential mechanism $M_E(x,u,R)$ selects and outputs an element $r\\in R$ with probability proportional to $exp({\\epsilon u(x,r)} / {2 \\Delta u})$. The exponential mechanism preserves  $(\\epsilon,0)$-differential privacy.\n",
        "\n",
        "#### **Sparse Vector Technique**\n",
        "\n",
        "lorem ipsum\n",
        "\n",
        "#### **Bernstein Mechanism**\n",
        "\n",
        "lorem ipsum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNLz3Duvb52i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4koh1BQqDYH",
        "colab_type": "text"
      },
      "source": [
        "### **Local DP**\n",
        "\n",
        "lorem ipsum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwYR_ec3b6iX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKYt3LXXqENQ",
        "colab_type": "text"
      },
      "source": [
        "### **Global DP**\n",
        "\n",
        "lorem ipsum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-MoffAIb7Jt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6Qz9_YajUY6",
        "colab_type": "text"
      },
      "source": [
        "### **Adversary**\n",
        "\n",
        "The nature of Differential Privacy is such that the epsilon-DP privacy guarantee holds even in the worst case.  \n",
        "\n",
        "Where the adversary knows everything about all the members of the database except the person of interest. And now it's up to the adversary to guess whether person-of-interest is in the database or not.  \n",
        "\n",
        "- DP ensures that no adversary modeling is required, as the DP guarantee caters to the worst case of adversary, where in, the adversary knows everything about the entire dataset, except the user of interest, and is still unable to infer anything.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qjqEdk2jUnG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEU91bgGUmv_",
        "colab_type": "text"
      },
      "source": [
        "### **Query Composition**\n",
        "\n",
        "privacy bounds when composing queries\n",
        "\n",
        "What happens when the same private database is queried multiple times? Are the privacy bounds compromised? Can we prove different theoretical privacy for this setting? Does the new bound depend on the number of queries? Can it be made independent of the number of queries?  \n",
        "\n",
        "**Basic Composition Theorem**\n",
        "\n",
        "Let $M_i : \\mathbb{N}^{|\\chi|} \\xrightarrow{} R_i$ be an $(\\epsilon_i, \\delta_i)$-differentially private algorithm for $i \\in [k]$. Then if $M_{[k]} : \\mathbb{N}^{|\\chi|} \\xrightarrow{} \\prod_{i=1}^k R_i$ is defined to be $M_{[k]}(x) = (M_1(x), M_2(x),...,M_k(x))$, then $M_{[k]}$ is $(\\sum_{i=1}^k \\epsilon_i, \\sum_{i=1}^k \\delta_i)$-differentially private.\n",
        "\n",
        "**Advanced Composition Theorem**\n",
        "\n",
        "For all $\\epsilon,\\delta,\\delta^{'} \\geq 0$, the class of $(\\epsilon,\\delta)$-differentially private mechanisms satisfy $(\\epsilon^{'},k\\delta + \\delta^{'})$-differential privacy under $k$-fold adaptive composition for:\n",
        "\\begin{equation}\n",
        "    \\epsilon^{,} = \\sqrt{2k ln(1/\\delta{'})}\\epsilon + k\\epsilon(e^\\epsilon - 1)\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tK62N-1mb7pb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bD1lwPqbdep",
        "colab_type": "text"
      },
      "source": [
        "### **Moments Accountant**\n",
        "\n",
        "lorem ipsum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBcjYTwwb8Zw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4Yzp56QqEx4",
        "colab_type": "text"
      },
      "source": [
        "### **Simple Use Case**\n",
        "\n",
        "lorem ipsum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEbHGSpeb9F6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "473PZqdGmDpE",
        "colab_type": "text"
      },
      "source": [
        "## **Applications**\n",
        "\n",
        "- \"Medical Imaging: Preserving patient privacy without the need for being a privacy expert where datasets from multiple medical organizations are combined for more accurate and diversified training data.\n",
        "- Healthcare + Internet of Things (e.g. heartrate monitoring using smart watches): Local differential privacy allows us to collect and perturb data on the user's device to ensure privacy.\n",
        "- Genomics: Differential Privacy allows us to protect against linkage attacks (where information in a public database overlaps with a sensitive dataset) and help ensure privacy for preset queries. \n",
        "- Geolocation and travel data: Mask the location of individuals and travel data in databases.\n",
        "- [COVID-WATCH ](https://www.covid-watch.org/): Enabling privacy preservation for a contact tracing app. Third parties would like to aggregate statistics based on users private information (like location, age, number of contact points etc...), but the information cannot leave the phone.\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJBvJTqIb95e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs5lThV6mDtg",
        "colab_type": "text"
      },
      "source": [
        "## **DP in Machine Learning**\n",
        "\n",
        "Overview"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8atxKyBb-_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07XyFIPtmDyO",
        "colab_type": "text"
      },
      "source": [
        "### **DP Classification**\n",
        "\n",
        "lorem ipsum\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75dJuZ0_b_bQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YelDGkItmEJd",
        "colab_type": "text"
      },
      "source": [
        "### **DP Regression**\n",
        "\n",
        "lorem ipsum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayMP98Ttb_7t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOHK4FW0mDly",
        "colab_type": "text"
      },
      "source": [
        "### **DP Clustering**\n",
        "\n",
        "lorem ipsum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLxnlRnOcAa3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7AQHZavWfWB",
        "colab_type": "text"
      },
      "source": [
        "## **DP at OpenMined**\n",
        "\n",
        "- Wrappers for existing libraries  \n",
        "- DifferentialPrivacy.ts "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5O1CeoZQcA3e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HepnFnB-l3Tt",
        "colab_type": "text"
      },
      "source": [
        "## **DP in SQL queries**\n",
        "\n",
        "@Ria (feel free to wipe out below suggestive key words)  \n",
        "\n",
        "Relational Algebra operations:  \n",
        "- select\n",
        "- from\n",
        "- join\n",
        "- where\n",
        "- group by "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnQE538Ml3a3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5DcLdLZkQLP",
        "colab_type": "text"
      },
      "source": [
        "## **Practical DP**\n",
        "\n",
        "### **Side-Channel Attacks**\n",
        "As explained above, DP pessimistically assumes that the attacker know all of the database contents except a single entry. \n",
        "However it also assumes that the only information gained by running a DP query is the query's output, and possibly the privacy budget consumed. \n",
        "This can lead to issues from 'side channels' where running a query gives an attacker more information than just the query result. \n",
        "The easiest way for this to happen is returning the privacy bugdet consumed when using a form of DP that ussumes just the result is returned. \n"
        "The next most common is through 'timing attacts' where the content of the database effects how long it takes a query to run. \n",
        "For example, a naive implmentation of sum above will perform a sum for every value above the one given and sum opperations take time. \n",
        "If we assume that the attacker knows every value in the database except one, then they can find its rank by seeing what queries take the time to run an 'extra' sum. \n"
        "Luckily these attacks can be mittigated in the code, and only matter for some use cases. \n",
        "For API's timing attacks matter. For researchers with full access deciding what results are safe to publish its not really an issue. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvqUzhplkQVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4mgWt3kerm1",
        "colab_type": "text"
      },
      "source": [
        "## **Things no one tells you about Differential Privacy**\n",
        "\n",
        "- looking into trailing ends of distributions\n",
        "- bounds on data\n",
        "- bounds on query output\n",
        "- bounds on noise distibution\n",
        "- floating point error privacy leakage\n",
        "- laplace formula implementation from formula using own little function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqQclh5Qx1jW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# looking into trailing ends of distributions\n",
        "# laplace formula implementation from formula using own little function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAITQxIjXJNJ",
        "colab_type": "text"
      },
      "source": [
        "## **Blog on DP**\n",
        "\n",
        "List of potential blogs:  \n",
        "- Walk through of DP with a simple example\n",
        "- Things no one tells you about DP\n",
        "- "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jH770CRcDNl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrxqbGbamWKG",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "## **Glossary**\n",
        "\n",
        "- \"*Local differential privacy:* Users perturb data before it is sent to a common server.\n",
        "- *Sensitivity of a query:* The amount a query’s results change when the database changes.\n",
        "- *Global sensitivity:* The maximum difference in a query’s result on any two neighboring databases.\n",
        "- *Local sensitivity:* The maximum difference between the query’s results on a true database and any neighbor of it. \n",
        "- *Sparse Vector technique:* Add noise and report only whether the noisy value exceeds the threshold. Privacy degrades only with the number of  queries which lie above the threshold, rather than with the total number of queries.\n",
        "- *Differentially  private  online  learning:* The goal in online learning is to design an algorithm that has the guarantee that \n",
        "  for all possible loss sequences l^≤T , even adversely chosen, the regret is guaranteed to tend to zero as T -> ∞.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfTkd0_tUo4s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Lf-D759mSSu",
        "colab_type": "text"
      },
      "source": [
        "## **Resources**\n",
        "\n",
        "- [Privacy-Preserving AI in Medical Imaging: Federated Learning, Differential Privacy, and Encrypted Computation](https://blog.openmined.org/federated-learning-differential-privacy-and-encrypted-computation-for-medical-imaging/)\n",
        "- [Use Cases of Differential Privacy](<https://blog.openmined.org/use-cases-of-differential-privacy/>)\n",
        "- [DP by Ted](<https://desfontain.es/privacy/>)\n",
        "- [DP at a Bird's eye view](<https://blog.openmined.org/privacy-preserving-ai-a-birds-eye-view/>)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbdHn9XXWN6K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uqB0RrnPdxA",
        "colab_type": "text"
      },
      "source": [
        "To do\n",
        "\n",
        "- figure out a few different \"levels\" of definitions. \n",
        "The definition we have is extremely concise, specific and accurate but we will need to have a version of that sentence that makes sense for more mainstream/less-savvy audiences\n",
        "- Adding use cases: Likely we will add one around COVID-Watch as well, medical\n",
        "- Section for providing headlines on when/where to use the various perturbation mechanisms\n",
        "- Use DP at bird's eye view "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsO1kMytcEYL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
